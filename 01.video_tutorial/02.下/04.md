


第⼗十讲：企业级监控数据采集脚本开发实

践

第⼗讲内容：
• pushgateway 的介绍

• pushgateway 的安装和运⾏和配置

• ⾃定义编写脚本的⽅法 发送pushgateway 采集

• 使⽤pushgateway的优缺点




（⼀） pushgateway 的介绍


pushgateway 是另⼀种采⽤被动推送的⽅式（⽽不是exporter 主动获取）获取监控数据的prometheus 插件


在上篇中 我们对pushgateway已经做过介绍


它是可以单独运⾏在 任何节点上的插件（并不⼀定要在被监 控客户端）


然后 通过⽤户⾃定义开发脚本 把需要监控的数据 发送给

pushgateway

然后pushgateway 再把数据 推送给prometheus server


接下来咱们来实践


（⼆）pushgatway 的安装和运⾏和配置


pushgateway 跟 prometheus和 exporter ⼀样


下载地址

https://prometheus.io/download/#pushgateway


解压后 直接运⾏

github的官⽅地址


https://github.com/prometheus/pushgateway 又是以go语⾔开发的， ⽤go开发的程序 在安装运⾏时 就是这 么得天独厚的 简单任性


同样 我们使⽤ daemonize ⽅式 讲pushgateway 放⼊后台运⾏


root 13980 13976 16 1⽉19 ? 4-19:53:51 /data/

pushgateway/pushgateway

root 13981 13978 0 1⽉19 ? 04:15:08 /data/pushgateway/

pushgateway1 -web.listen-address 0.0.0.0:9092


关于pushgateway的配置 主要指的是 在prometheus sever端的配 置


我们来看下


image


在prometheus.yml 配置⽂件中， 单独定义⼀个job 然后 target 指向到 pushgateway运⾏所在的 机器名和 pushgateway运⾏的端⼜ 即可


这⾥我们发现了没有， localhost:9091/9092 ⼤⽶⽼师企业中 使⽤的 pushgateway开启了两个

在prometheus_server 本机上


为什么开启两个 我最后⼀个段落给⼤家解释


（三）⾃定义编写脚本的⽅法 发送pushgateway 采集 接下来 进⼊重头戏了

pushgateway 本⾝是没有任何抓取监控数据的功能的 它只是被 动的等待推送过来

所以 让我们来学习 pushgateway 编程脚本的写法 如下是⼀段 ⽣产环境中 使⽤shell 编写的 pushgateway脚本

⽤于抓取 TCP waiting_connection 瞬时数量


cat /usr/local/node_exporter/node_exporter_shell.sh


#!/bin/bash


instance_name=`hostname -f | cut -d'.' -f1` #本机机器名 变量

⽤于之后的 标签


if [ $instance_name == "localhost" ];then # 要求机器名 不能是

localhost 不然标签就没有区分了

echo "Must FQDN hostname" exit 1

fi


# For waitting connections

label="count_netstat_wait_connections" # 定⼀个新的 key

count_netstat_wait_connections=`netstat -an | grep -i wait | wc -l`

#定义⼀个新的数值 netstat中 wait 的数量


echo "$label : $count_netstat_wait_connections"


echo "$label $count_netstat_wait_connections" | curl --data- binary @- http://prometheus.server.com:9091/metrics/job/ pushgateway1/instance/$instance_name


#最后 把 key & value 推送给 pushgatway


curl —data-binary

将HTTP POST请求中的数据发送给HTTP服务器器(pushgateway)， 与⽤用户提交HTML表单时浏览器器的⾏行行为完全⼀一样。

HTTP POST请求中的数据为纯⼆二进制数据


从上⾯的 短短的⼗⼏⾏的脚本来看 其实最重要的 是两个标红的地⽅

count_netstat_wait_connections=`netstat -an | grep -i wait | wc -l`


后⾯红⾊部分 是我们通过Linux命令⾏ 就简单的获取到了 我 们需要监控的数据 TCP_WAIT数

http://prometheus.server.com:9091/metrics/job/pushgateway1/ instance/$instance_name


最后这⾥ ⽤POST ⽅式 把 key & value 推送给 pushgatway的URL地址 这个URL地址中 分成如下三个部分

http://prometheus.server.com:9091/metrics/job/pushgateway1

这⾥是 URL的主location


job/pushgateway1

这⾥是 第⼆部分 第⼀个标签： 推送到 哪⼀个prometheus.yml 定义的 job⾥


{instance=“server01"} instance/$instance_name

这⾥是 第⼆个标签 推送后 显⽰的 机器名是什么


其实 pushgateway的脚本 并不是很难写 但是通过这样的脚本编程⽅式

我们可以很快速的 ⾃定义 我们需要的任何 监控数据(Linux 命 令⽅式)


最后 这个我们编写的监控bash脚本 是⼀次性执⾏的 bash script.sh

我们需要按时间段 反复执⾏


所以呢？⾃然就得结合 contab了

这⾥顺带提⼀句


crontab 默认 只能最短⼀分钟的间隔 如果希望 ⼩于⼀分钟的间隔 15s


我们使⽤如下的⽅法


sleep 10

sleep 20

image

😃


image

之后 我们回到prometheus主界⾯ 尝试输⼊ 由我们⾃⼰定义的

new_key

看看结果


key的名字 就是这个 label="count_netstat_wait_connections"

# 定⼀个新的 key


image


嘿嘿 很愉快 对不？


其他种类的监控数据 我们都可以通过类似的形式 直接写脚本 发送

使⽤python 也是很好的⽅式

不过我个⼈还是推荐 bash shell 因为速度超级的快～ （⽤

image

python的话 还得include 很多库 ⼤⽶⽼师有点懒 哈哈😄 ）


（四） 使⽤pushgateway的优缺点


⼤⽶之前就跟⼤家说过 pushgateway这种⾃定义的 采集⽅式

⾮常的快速 ⽽且极其灵活 ⼏乎不收到任何约束


其实我个⼈ 还是⾮常希望 使⽤pushgateway来获取监控数据的


各类的exporters虽然玲琅满⽬ ⽽且默认提供的数据很多了已 经


⼀般情况下 我在企业中 只安装 node_exporter 和 DB_exporter

两个


其他种类的 监控数据 我倾向于 全部使⽤pushgateway的⽅式 采集 （要的就是快速～ 灵活~）

不过⾔归正传 习惯并不代表正确性


pushgateway虽然灵活 但是 也是存在⼀些 短板的 接下来 我们来看看 prometheus官⽹是怎么说的


WHEN TO USE THE PUSHGATEWAY



• Should I be using the Pushgateway?

 
• Alternative strategies

  

The Pushgateway is an intermediary service which allows you to push metrics from jobs which cannot be scraped. For details, see Pushing metrics.

Should I be using the Pushgateway?

We only recommend using the Pushgateway in certain limited cases. There are several pitfalls when blindly using the Pushgateway instead of Prometheus's usual pull model for general metrics collection:
• When monitoring multiple instances through a single Pushgateway, the Pushgateway becomes both a single point of failure and a potential bottleneck.

• You lose Prometheus's automatic instance health monitoring via the up metric (generated on every scrape).

• The Pushgateway never forgets series pushed to it and will expose them to Prometheus forever unless those series are manually deleted via the Pushgateway's API.


The latter point is especially relevant when multiple instances of a job diﬀerentiate their metrics in the Pushgateway via

an instance label or similar. Metrics for an instance will then

remain in the Pushgateway even if the originating instance is

renamed or removed. This is because the lifecycle of the Pushgateway as a metrics cache is fundamentally separate from the lifecycle of the processes that push metrics to it. Contrast this to Prometheus's usual pull-style monitoring: when an instance disappears (intentional or not), its metrics will automatically disappear along with it. When using the Pushgateway, this is not the case, and you would now have to delete any stale metrics manually or automate this lifecycle synchronization yourself.

Usually, the only valid use case for the Pushgateway is for capturing the outcome of a service-level batch job. A "service- level" batch job is one which is not semantically related to a specific machine or job instance (for example, a batch job that deletes a number of users for an entire service). Such a job's metrics should not include a machine or instance label to decouple the lifecycle of specific machines or instances from the pushed metrics. This decreases the burden for managing stale metrics in the Pushgateway. See also the best practices for monitoring batch jobs.

Alternative strategies

If an inbound firewall or NAT is preventing you from pulling metrics from targets, consider moving the Prometheus server behind the network barrier as well. We generally recommend running Prometheus servers on the same network as the monitored instances.

For batch jobs that are related to a machine (such as automatic security update cronjobs or configuration management client runs), expose the resulting metrics using the Node Exporter's textfile module instead of the Pushgateway.

⼀⼤堆的英⽂ 其实总结起来 基本上两点


1） pushgateway 会形成⼀个单点瓶颈，假如好多个 脚本同时 发送给 ⼀个pushgateway的进程 如果这个进程没了，那么监控数据也就没了


2） pushgateway 并不能对发送过来的 脚本采集数据 进⾏更智 能的判断 假如脚本中间采集出问题了

那么有问题的数据 pushgateway⼀样照单全收 发送给 prometheus

虽然有这么两个所谓的缺点 但是实际上通过我 2年多的使⽤

对于第⼀条缺点 其实只要服务器不当机 那么基本上

pushgateway运⾏还是很稳定的

就算有太多的脚本 同时都发送给⼀个pushgateway 其实也只是 接收的速度会变慢 丢数据没有遇到过 （有的时候 我甚⾄觉得

⽐exporters还稳定 exporters倒是 有时候会真的就当机 或者 丢 失数据 因为exporters开发⽐较复杂 越简单的东西 往往越稳 定）


对于第⼆条缺点

其实只要我们在写脚本的时候 细⼼⼀些 别出错（这也是为什 么 我推荐就⽤ bash 写 因为简单不容易出错 ， python我可不 敢打包票）

那么 监控数据也不会有错误

所以说呢 相信⼤⽶ 我们就放⼼的使⽤pushgateway来尽情采集

我们的数据吧


课堂的结尾 给⼤家留⼀个作业吧（学习需要勤快）


我们⾃⼰搭建pushgateway 并⾃⾏编写⼀个 采集脚本 （bash shell即可）

采集⼀个 ping 延迟和丢包率吧 ^_^


image



image


image


第⼗十⼀一

讲：Prometheus 之 exporter模块源代码示例例


本节课内容
• 编写⼀个exporter的流程


• ⽤go开发⼀个exporter的源代码 及讲解

• 对于编写exporter的个⼈建议

（⼀）编写⼀个exporter的流程 官⽹有介绍 编写exporter的详细内容 我们这⾥给⼤家做⼀个 简单的归纳

⾸先 不同于pushgateway, exporter是⼀个独⽴运⾏ 的采集程序 其中的功能需要有这三个部分

1） ⾃⾝是HTTP 服务器，可以响应 从外发过来的 HTTP GET

请求

2） ⾃⾝需要运⾏在后台，并可以定期触发 抓取本地的监控 数据

3） 返回给prometheus_server 的内容 是需要符合 prometheus

规定的metrics 类型（Key-Value）

4) key-value -> prometheus(TS) values( float int ) return string *

nil


接下来 给⼤家⼀份我之前⽤ go编写⼀份 exporter源代码 (⼆) ⽤go开发⼀个exporter的源代码 及讲解


package main


import ( "fmt"

_"net/http/pprof" "sync"

// "github.com/hpcloud/tail" "github.com/prometheus/client_golang/prometheus" "github.com/prometheus/common/log" "github.com/prometheus/common/version"

// //"net" "net/http"

// "net/url"

// "errors" "flag"

"os" "os/exec" "io/ioutil"

// "regexp"

// "strconv"

// "strings" "time"

)


const (

namespace = "my_exporter"

)


type Exporter struct { cmd string

mutex sync.RWMutex up,freeRam,TotalRam prometheus.Gauge Debug bool

upstreamResponseTimes *prometheus.HistogramVec


}


func (e *Exporter) scrape() {


log.Infoln("--------") e.up.Set(1)

}


func (e *Exporter) Describe(ch chan<- *prometheus.Desc) { e.upstreamResponseTimes.Describe(ch)

}

func (e *Exporter) Collect(ch chan<- prometheus.Metric) {

e.m utex.Lock() // To protect metrics from concurrent collects defer e.mutex.Unlock()


e.scrape() e.upstreamResponseTimes.Collect(ch) e.up.Collect(ch)

}


func NewExporter(cmd string, timeout time.Duration) (*Exporter, error) {

return &Exporter{ cmd: cmd,

up: prometheus.NewGauge(prometheus.GaugeOpts{ Namespace: namespace,

Name: "up",

Help: "Was the last scrape of nginx exporter successful.",

}),

upstreamResponseTimes: prometheus.NewHistogramVec( prometheus.HistogramOpts{

Namespace: namespace,

Name: "http_upstream_request_times",

Help: "Response times from backends/upstreams",

},

[]string{"method", "endpoint", "response_code", "client_type"},

),

}, nil

}


func run() {


cmd := exec.Command("/bin/sh", "-c", `free -m `) stdout , err := cmd.StdoutPipe()

if err != nil { panic(err.Error())

}


if err != nil {

fmt.Println("Stdoutpipe error", err.Error())

}


stderr , err := cmd.StderrPipe()


if err != nil {

fmt.Println("Stderrpipe error", err.Error())

}


if err := cmd.Start(); err != nil { fmt.Println("error start", err )


}


bytesErr, err := ioutil.ReadAll(stderr)

if err != nil {

fmt.Println("ReadAll stderr: ", err.Error()) return

}


if len(bytesErr) != 0 {

fmt.Printf("stderr is not nil: %s", bytesErr) return

}


bytes, err := ioutil.ReadAll(stdout) if err != nil {

fmt.Println("ReadAll stdout: ", err.Error()) return

}


if err := cmd.Wait(); err != nil { fmt.Println("Wait: ", err.Error()) return

}


fmt.Printf("stdout: %s", string(bytes))


}


func main() { var (

listenAddress = flag.String("web.listen-address", ":9101", "Address to listen on for web interface and telemetry.")

metricsPath = flag.String("web.telemetry-path", "/metrics", "Path under which to expose metrics.")

cmd = flag.String("free", "", "command to run")

nginxTimeout = flag.Duration("nginx.timeout", 5*time.Second, "Timeout for trying to get stats from HA.")

showVersion = flag.Bool("version", false, "Print version information.")

// debug = flag.Bool("debug", false, "Print debug information.")

)

flag.Parse()


if *showVersion {

fmt.Println(os.Stdout, version.Print("shannon_exporter")) os.Exit(0)

}


log.Infoln("Starting shannon_exporter", version.Info()) log.Infoln("Build context", version.BuildContext())


run()


exporter, _ := NewExporter(*cmd, *nginxTimeout)


prometheus.MustRegister(exporter) prometheus.MustRegister(version.NewCollector("nginx_exporte r"))

http.Handle(*metricsPath, prometheus.Handler()) http.HandleFunc("/", func(w http.ResponseWriter, r

*http.Request) { w.Write([]byte(`<html>

<head><title>Haproxy Exporter</title></head>

<body>

<h1>Haproxy Exporter</h1>

<p><a href='` + *metricsPath + `'>Metrics</a></p>

</body>

</html>`))

})


log.Fatal(http.ListenAndServe(*listenAddress, nil))


}


// 前⾯的 scrape describe Collect 是struct类型的成员函数，这⼏ 个函数并没有直接在这个go⾥被调⽤，⽽是 MustRegister注册 进去了它们。


// http.Handle ⾥的prometheus.Handler 将上⼀部2个Mustregister 的注册 关联进⼊http.handle. 也就进⼀步注册进⼊了httpserver- listerner


// http.lister是组赛程序，只有启动后 被curl的时候 才会执⾏。 并且由于之前经过2步的注册，跟exporter的成员函数建⽴了 关联（其实是指针关联） 所以，每次被curl的时候 所有上⾯ 的成员函数都会被调⽤

// 例如上⾯的up.set(1) 就是只有被curl的时候才会被调⽤，成

员函数是作为 ⽣成metrics的⼊⼜


注： 本Go exporter 源代码 是半年前写的了 其中⼀些地⽅ 可 能需要进⼀步修改才可以正常使⽤ （本⼈go开发能⼒也⽐较 菜 - _ -）


（三） 个⼈建议


通过上⾯的对⼀个exporter源代码的讲解 我们也看得出来

编写⼀个 exporter 远远⽐ 写⼀个pushgateway脚本要复杂的多 的多


个⼈建议：除⾮是⼯作中真的有需要（例如：社区的exporters 都不能满⾜需求，且对于监控客户端的规范化⽐较严格） 且 对⾃⼰的编程能⼒有⾃信

那么可以⾃⾏开发new exporter


exporter-prometheus 5s GET => exporter Handler-> handler-lister 开发的过程中 要特别注意 各种⽂件句柄等等资源的 完整回 收

因为⼀个⾮常被频繁调⽤的后台程序中 ⼀旦循环中出现资源 泄漏 那么后果是很严重的

（之前其实就遇到过⼀次 资源没有正确回收 导致每⼀次 promethue_server访问过来 都造成僵⼫进程 最终服务器被拖垮 的窘境 …囧 当然了 如果是 开发⾼⼿的话 相信我的担⼼也就 多余啦～～）

所以 如果企业中 运维开发的实⼒不够过硬，我还是尽量建议

⼤家 使⽤现成的社区exporters, 配合⾃⾏开发pushgateway脚本

image

😄


image


image


image


